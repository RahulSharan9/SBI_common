import os
import pandas as pd


# ========================
# USER CONFIG
# ========================
output_dir = "output_folder"   # change if needed

# column aliases we want to normalize
COLUMN_ALIASES = {
    "file": "file",
    "column": "column",
    "min_value": "min",
    "max_value": "max",
    "minlen": "min_length",
    "min_len": "min_length",
    "min_length": "min_length",
    "maxlen": "max_length",
    "max_len": "max_length",
    "max_length": "max_length",
    "min_date": "min_date",
    "max_date": "max_date",
    "avg_value": "avg_value",
    "mean_value": "avg_value",        # alias
    "total_rows": "total_rows"
}


# ========================
# Weighted mean helper
# ========================
def weighted_mean(series, weights):
    if weights.sum() == 0:
        return pd.NA
    return (series * weights).sum() / weights.sum()


# ========================
# STEP 1: LOAD ALL STATS
# ========================
all_data = []

for quarter_folder in os.listdir(output_dir):
    quarter_path = os.path.join(output_dir, quarter_folder)
    if not os.path.isdir(quarter_path):
        continue

    data_quality_path = os.path.join(quarter_path, "data_quality")
    if not os.path.isdir(data_quality_path):
        continue

    csv_files = [f for f in os.listdir(data_quality_path) if f.endswith("_stats.csv")]
    if not csv_files:
        continue

    for csv_file in csv_files:
        csv_path = os.path.join(data_quality_path, csv_file)
        df = pd.read_csv(csv_path)

        # Normalize headers
        df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')

        # Rename using aliases (tolerant)
        for old, new in COLUMN_ALIASES.items():
            if old.lower() in df.columns:
                df.rename(columns={old.lower(): new}, inplace=True)

        # ensure essential columns exist
        for col in ["min", "max", "avg_value", "min_length", "max_length", "min_date", "max_date"]:
            if col not in df.columns:
                df[col] = pd.NA

        # convert dtypes
        for col in ["min", "max", "avg_value", "min_length", "max_length"]:
            df[col] = pd.to_numeric(df[col], errors='coerce')

        for col in ["min_date", "max_date"]:
            df[col] = pd.to_datetime(df[col], errors='coerce')

        # segment from filename
        df["segment"] = df["file"].str.replace(".parquet", "", regex=False)
        df["Quarter"] = quarter_folder

        all_data.append(df)


# ========================
# STEP 2: COMBINE QUARTERS
# ========================
master_df = pd.concat(all_data, ignore_index=True)


# ========================
# STEP 3: GROUP BY SEGMENT + COLUMN
# ========================
grouped = master_df.groupby(["segment", "column"])


# ========================
# Helper to track which quarter had min/max
# ========================
def quarter_for_extreme(df, group, col, func):
    if group[col].isnull().all():
        return pd.NA
    idx = group[col].idxmin() if func == "min" else group[col].idxmax()
    return df.loc[idx, "Quarter"]


# ========================
# STEP 4: Aggregate global min/max
# ========================
summary = grouped.agg({
    "min": "min",
    "max": "max",
    "min_length": "min",
    "max_length": "max",
    "min_date": "min",
    "max_date": "max"
}).reset_index()

summary = summary.round(2)


# ========================
# STEP 5: Add quarters for each extreme value
# ========================
summary["min_quarter"] = summary.apply(
    lambda row: quarter_for_extreme(master_df, grouped.get_group((row["segment"], row["column"])), "min", "min"),
    axis=1
)
summary["max_quarter"] = summary.apply(
    lambda row: quarter_for_extreme(master_df, grouped.get_group((row["segment"], row["column"])), "max", "max"),
    axis=1
)

summary["min_length_quarter"] = summary.apply(
    lambda row: quarter_for_extreme(master_df, grouped.get_group((row["segment"], row["column"])), "min_length", "min"),
    axis=1
)
summary["max_length_quarter"] = summary.apply(
    lambda row: quarter_for_extreme(master_df, grouped.get_group((row["segment"], row["column"])), "max_length", "max"),
    axis=1
)

summary["min_date_quarter"] = summary.apply(
    lambda row: quarter_for_extreme(master_df, grouped.get_group((row["segment"], row["column"])), "min_date", "min"),
    axis=1
)
summary["max_date_quarter"] = summary.apply(
    lambda row: quarter_for_extreme(master_df, grouped.get_group((row["segment"], row["column"])), "max_date", "max"),
    axis=1
)


# ========================
# STEP 6: Weighted mean across quarters
# ========================
wmeans = (
    master_df.groupby(["segment", "column"])
    .apply(lambda g: weighted_mean(g["avg_value"], g["total_rows"]))
    .reset_index()
    .rename(columns={0: "weighted_mean"})
)

summary = summary.merge(wmeans, on=["segment", "column"], how="left")
summary["weighted_mean"] = summary["weighted_mean"].round(2)


# ========================
# STEP 7: Order columns nicely
# ========================
final_cols = [
    "segment", "column",
    "min", "min_quarter",
    "max", "max_quarter",
    "min_length", "min_length_quarter",
    "max_length", "max_length_quarter",
    "min_date", "min_date_quarter",
    "max_date", "max_date_quarter",
    "weighted_mean"
]

summary = summary[final_cols]


# ========================
# STEP 8: Save final output
# ========================
summary.to_csv("segment_value_trends_with_quarters.csv", index=False)
print("\nâœ… Saved: segment_value_trends_with_quarters.csv\n")
